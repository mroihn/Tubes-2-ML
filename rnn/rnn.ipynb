{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evcxRrSePUJe"
      },
      "source": [
        "A code to train sentiment analysis for NusaX dataset.\n",
        "\n",
        "Simply `runtime > run all` to train and test.\n",
        "Modify the language on the bottom part of this code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esRErPgWTO7W"
      },
      "source": [
        "# Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bytkulDT2MfD",
        "outputId": "89f78e94-e8c7-4a77-d69c-50bdf33218b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nusax'...\n",
            "remote: Enumerating objects: 301, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 301 (delta 4), reused 2 (delta 2), pack-reused 296 (from 1)\u001b[K\n",
            "Receiving objects: 100% (301/301), 3.74 MiB | 14.85 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n"
          ]
        }
      ],
      "source": [
        "# grab the data first\n",
        "!git clone https://github.com/IndoNLP/nusax.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvHinXPi2WDF",
        "outputId": "e94d9adb-ff47-4aee-c8af-1f2a6f6c1327"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# read csv data\n",
        "# return a pair of (list of data, list of label)\n",
        "# also tokenize the input first\n",
        "def load_data(filedir):\n",
        "    df = pd.read_csv(filedir)\n",
        "    data = list(df['text'])\n",
        "    data = [\" \".join(word_tokenize(sent)) for sent in data]\n",
        "    print(list(df['label']))\n",
        "    return (data, list(df['label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ioU2Qx3kNX",
        "outputId": "6d334bd4-a0f1-4b23-dc1c-68353cd963fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "from scipy.sparse import vstack\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def hyperparam_tuning(xtrain, ytrain, xvalid, yvalid, classifier, param_grid):\n",
        "    # combine train and valid\n",
        "    x = vstack([xtrain, xvalid])\n",
        "    y = ytrain + yvalid\n",
        "\n",
        "    # create predefined split\n",
        "    # -1 for all training and 0 for all validation\n",
        "    ps = PredefinedSplit([-1] * len(ytrain) + [0] * len(yvalid))\n",
        "    clf = GridSearchCV(classifier, param_grid, cv = ps)\n",
        "    clf = clf.fit(x, y)\n",
        "\n",
        "    return clf\n",
        "\n",
        "\n",
        "def train_and_test(lang, directory=\"/content/nusax/datasets/sentiment/\", feature=\"BoW\", classifier=\"nb\"):\n",
        "    xtrain, ytrain = load_data(directory + lang +\"/train.csv\")\n",
        "    xvalid, yvalid = load_data(directory + lang + \"/valid.csv\")\n",
        "    xtest, ytest = load_data(directory + lang + \"/test.csv\")\n",
        "\n",
        "    # train feature on train data\n",
        "    if feature == \"bow\":\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif feature == \"tfidf\":\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    else:\n",
        "        raise Exception('Vectorizer unknown. Use \"BoW\" or \"tfidf\"')\n",
        "    vectorizer.fit(xtrain)\n",
        "\n",
        "    # transform\n",
        "    xtrain = vectorizer.transform(xtrain)\n",
        "    xvalid = vectorizer.transform(xvalid)\n",
        "    xtest = vectorizer.transform(xtest)\n",
        "\n",
        "    # all classifiers\n",
        "    classifier_model = {\"nb\" : MultinomialNB(),\n",
        "                        \"svm\": SVC(),\n",
        "                        \"lr\" : LogisticRegression(),\n",
        "                       }\n",
        "    # all params for grid-search\n",
        "    param_grids = {\"nb\" : {\"alpha\": np.linspace(0.001,1,50)},\n",
        "                   \"svm\": {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['rbf', 'linear']},\n",
        "                   \"lr\" : {'C': np.linspace(0.001,10,100)},\n",
        "                  }\n",
        "\n",
        "    clf = hyperparam_tuning(xtrain, ytrain, xvalid, yvalid,\n",
        "                            classifier=classifier_model[classifier],\n",
        "                            param_grid=param_grids[classifier])\n",
        "\n",
        "    pred = clf.predict(xtest.toarray())\n",
        "    f1score = f1_score(ytest,pred, average='macro')\n",
        "\n",
        "    return f1score, clf, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SCOGyp9TT7_"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4eQmj054Z4M",
        "outputId": "80800dd4-b1c3-4154-d0c6-708506eb2ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for sentiment analysis classifier indonesian\n",
            "['neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'positive', 'negative', 'neutral', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'negative', 'neutral', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'negative', 'neutral', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'neutral', 'negative', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'neutral', 'negative', 'positive', 'negative', 'negative', 'neutral', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'negative', 'neutral', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'positive', 'positive', 'negative', 'negative', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'neutral', 'neutral', 'neutral', 'negative', 'positive', 'positive', 'negative', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'positive', 'neutral', 'positive', 'negative', 'positive', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'negative', 'negative', 'negative', 'neutral', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'negative', 'neutral', 'neutral', 'negative', 'negative', 'negative', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'neutral', 'positive', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'negative', 'negative', 'neutral', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'neutral', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'neutral', 'negative', 'negative', 'positive', 'neutral', 'neutral', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'neutral', 'negative', 'neutral', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'positive', 'positive', 'neutral', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'neutral', 'neutral', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative']\n",
            "['neutral', 'negative', 'neutral', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'negative', 'positive', 'positive', 'negative', 'neutral', 'negative', 'neutral', 'neutral', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'neutral', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'negative', 'positive', 'negative', 'positive', 'neutral', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'neutral', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'neutral', 'positive', 'positive', 'positive', 'neutral']\n",
            "['positive', 'neutral', 'negative', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'positive', 'neutral', 'negative', 'negative', 'neutral', 'negative', 'negative', 'neutral', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'negative', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'negative', 'negative', 'negative', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'neutral', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'negative', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'negative', 'negative', 'positive', 'negative', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'neutral', 'negative', 'positive', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'positive', 'negative', 'positive', 'negative', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'negative', 'negative', 'negative', 'neutral', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'negative', 'neutral', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'neutral', 'negative', 'negative', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'negative', 'neutral', 'negative', 'negative', 'negative', 'positive', 'neutral', 'negative', 'positive', 'negative', 'negative', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'neutral', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'neutral', 'negative', 'positive', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'positive', 'neutral', 'negative', 'negative', 'negative', 'neutral', 'negative', 'neutral', 'negative', 'negative', 'negative', 'negative', 'neutral', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'neutral', 'positive', 'negative', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'negative', 'positive', 'negative', 'neutral', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'neutral', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'neutral', 'negative', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative']\n",
            "Training done. F1 on test set is 0.7311052022751223\n",
            "\n",
            "Sentiment on the input text is ['neutral']\n"
          ]
        }
      ],
      "source": [
        "#@title Sentiment analysis demo\n",
        "language = \"indonesian\" #@param [\"indonesian\", \"english\", \"javanese\", \"sundanese\", \"balinese\", \"madurese\", \"minangkabau\", \"toba_batak\", \"acehnese\", \"buginese\", \"ngaju\", \"banjarese\"]\n",
        "input_sentiment = \"abang saya keterima kerja di kamboja\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "print(f\"Training for sentiment analysis classifier {language}\")\n",
        "f1, clf, vectorizer = train_and_test(language, feature=\"bow\")\n",
        "print(f\"Training done. F1 on test set is {f1}\")\n",
        "\n",
        "input_sentiment = \" \".join(word_tokenize(input_sentiment))\n",
        "sent = clf.predict(vectorizer.transform([input_sentiment]).toarray())\n",
        "print(f\"\\nSentiment on the input text is {sent}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-voEMFWqt6L",
        "outputId": "5b5b0e32-5254-4ca6-a394-10403a0d9e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - loss: 1.1497 - sparse_categorical_accuracy: 0.3724 - val_loss: 1.0860 - val_sparse_categorical_accuracy: 0.4100\n",
            "Epoch 2/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 1.0739 - sparse_categorical_accuracy: 0.4865 - val_loss: 1.1597 - val_sparse_categorical_accuracy: 0.3700\n",
            "Epoch 3/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 1.1372 - sparse_categorical_accuracy: 0.4221 - val_loss: 1.1151 - val_sparse_categorical_accuracy: 0.4300\n",
            "Epoch 4/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 0.9445 - sparse_categorical_accuracy: 0.5819 - val_loss: 1.2252 - val_sparse_categorical_accuracy: 0.3200\n",
            "Epoch 5/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.8317 - sparse_categorical_accuracy: 0.6551 - val_loss: 1.2125 - val_sparse_categorical_accuracy: 0.4200\n",
            "Epoch 6/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.6023 - sparse_categorical_accuracy: 0.7711 - val_loss: 1.4366 - val_sparse_categorical_accuracy: 0.3900\n",
            "Epoch 7/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.3176 - sparse_categorical_accuracy: 0.9033 - val_loss: 1.7323 - val_sparse_categorical_accuracy: 0.4100\n",
            "Epoch 8/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.1597 - sparse_categorical_accuracy: 0.9535 - val_loss: 1.8240 - val_sparse_categorical_accuracy: 0.4300\n",
            "Epoch 9/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0536 - sparse_categorical_accuracy: 0.9933 - val_loss: 2.1854 - val_sparse_categorical_accuracy: 0.4400\n",
            "Epoch 10/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0384 - sparse_categorical_accuracy: 0.9982 - val_loss: 2.3375 - val_sparse_categorical_accuracy: 0.4000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
            "Macro F1-score on test: 0.3489\n"
          ]
        }
      ],
      "source": [
        "# 1. Data Preparation and Preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def load_nusax_as_dataframe(base_dir, lang=\"indonesian\"):\n",
        "    train = pd.read_csv(f\"{base_dir}/{lang}/train.csv\")\n",
        "    valid = pd.read_csv(f\"{base_dir}/{lang}/valid.csv\")\n",
        "    test = pd.read_csv(f\"{base_dir}/{lang}/test.csv\")\n",
        "    return train, valid, test\n",
        "\n",
        "# 2. Tokenization and Embedding Preparation\n",
        "def preprocess_text(train, valid, test, max_tokens=10000, seq_len=100):\n",
        "    vectorizer = TextVectorization(\n",
        "        max_tokens=max_tokens,\n",
        "        output_sequence_length=seq_len,\n",
        "        standardize=\"lower_and_strip_punctuation\"\n",
        "    )\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(train[\"text\"]).batch(128)\n",
        "    vectorizer.adapt(text_ds)\n",
        "    x_train = vectorizer(np.array(train[\"text\"]))\n",
        "    x_valid = vectorizer(np.array(valid[\"text\"]))\n",
        "    x_test = vectorizer(np.array(test[\"text\"]))\n",
        "    return x_train, x_valid, x_test, vectorizer\n",
        "\n",
        "# 3. Build RNN Model (Keras)\n",
        "def build_rnn_model(vocab_size, seq_len, embed_dim, rnn_units, num_layers=1, dropout_rate=0.3, \n",
        "                    bidirectional=False, num_classes=3):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=seq_len, name=\"embedding\"))\n",
        "    for _ in range(num_layers):\n",
        "        rnn_layer = SimpleRNN(rnn_units, return_sequences=True if _ < num_layers-1 else False)\n",
        "        if bidirectional:\n",
        "            rnn_layer = Bidirectional(rnn_layer)\n",
        "        model.add(rnn_layer)\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation=\"softmax\", name=\"classifier\"))\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        optimizer=Adam(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 4. Training and Experimentation\n",
        "def train_and_evaluate_rnn(x_train, y_train, x_valid, y_valid, x_test, y_test, \n",
        "                           vocab_size, seq_len, embed_dim, rnn_units, num_layers, dropout_rate, bidirectional, num_classes):\n",
        "    model = build_rnn_model(vocab_size, seq_len, embed_dim, rnn_units, num_layers, dropout_rate, bidirectional, num_classes)\n",
        "    model.fit(x_train, y_train, epochs=10, validation_data=(x_valid, y_valid), batch_size=32)\n",
        "\n",
        "    model.save_weights(\"nusax_rnn.weights.h5\")\n",
        "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "    print(f\"Macro F1-score on test: {f1:.4f}\")\n",
        "    return model, f1, y_pred\n",
        "\n",
        "# 5. Keras Usage:\n",
        "train, valid, test = load_nusax_as_dataframe(\"nusax/datasets/sentiment\")\n",
        "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "y_train = train[\"label\"].map(label_map).values\n",
        "y_valid = valid[\"label\"].map(label_map).values\n",
        "y_test = test[\"label\"].map(label_map).values\n",
        "x_train, x_valid, x_test, vectorizer = preprocess_text(train, valid, test, max_tokens=10000, seq_len=100)\n",
        "\n",
        "model, f1, y_pred = train_and_evaluate_rnn(\n",
        "    x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
        "    vocab_size=10000, seq_len=100, embed_dim=64, rnn_units=64, num_layers=2, dropout_rate=0.3, bidirectional=False, num_classes=3\n",
        ")\n",
        "\n",
        "# 6. FROM SCRATCH IMPLEMENTATION\n",
        "# ---- For Each Layer ----\n",
        "class MyEmbedding:\n",
        "    def __init__(self, weights):\n",
        "        self.weights = weights\n",
        "    def forward(self, x):\n",
        "        x = np.asarray(x, dtype=np.int32)\n",
        "        return self.weights[x]\n",
        "\n",
        "class MySimpleRNN:\n",
        "    def __init__(self, Wx, Wh, b, return_sequences=False):\n",
        "        self.Wx = Wx\n",
        "        self.Wh = Wh\n",
        "        self.b = b\n",
        "        self.return_sequences = return_sequences\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq, embed_dim)\n",
        "        h = np.zeros((x.shape[0], self.Wh.shape[0]), dtype=np.float32)\n",
        "        outputs = []\n",
        "        for t in range(x.shape[1]):\n",
        "            h = np.tanh(x[:, t] @ self.Wx + h @ self.Wh + self.b)\n",
        "            if self.return_sequences:\n",
        "                outputs.append(h.copy())\n",
        "        if self.return_sequences:\n",
        "            return np.stack(outputs, axis=1)\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "class MyDropout:\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class MyDense:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "    def forward(self, x):\n",
        "        return x @ self.W + self.b\n",
        "\n",
        "class MySoftmax:\n",
        "    def forward(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "class MyRNNModel:\n",
        "    def __init__(self, keras_model):\n",
        "        self.embedding = MyEmbedding(keras_model.get_layer(\"embedding\").get_weights()[0])\n",
        "        self.rnn_layers = []\n",
        "        self.dropout_layers = []\n",
        "        last_is_rnn = False\n",
        "        for layer in keras_model.layers:\n",
        "            if \"SimpleRNN\" in layer.__class__.__name__:\n",
        "                Wx, Wh, b = layer.get_weights()\n",
        "                self.rnn_layers.append(MySimpleRNN(Wx, Wh, b, return_sequences=layer.return_sequences))\n",
        "                last_is_rnn = True\n",
        "            elif \"Dropout\" in layer.__class__.__name__:\n",
        "                self.dropout_layers.append(MyDropout(layer.rate))\n",
        "                last_is_rnn = False\n",
        "\n",
        "        self.ordered_layers = []\n",
        "        rnn_idx = 0\n",
        "        dropout_idx = 0\n",
        "        for layer in keras_model.layers:\n",
        "            if \"Embedding\" in layer.__class__.__name__:\n",
        "                continue  # handled above\n",
        "            elif \"SimpleRNN\" in layer.__class__.__name__:\n",
        "                self.ordered_layers.append(self.rnn_layers[rnn_idx])\n",
        "                rnn_idx += 1\n",
        "            elif \"Dropout\" in layer.__class__.__name__:\n",
        "                self.ordered_layers.append(self.dropout_layers[dropout_idx])\n",
        "                dropout_idx += 1\n",
        "        # Dense and softmax\n",
        "        dense_W, dense_b = keras_model.get_layer(\"classifier\").get_weights()\n",
        "        self.dense = MyDense(dense_W, dense_b)\n",
        "        self.softmax = MySoftmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding.forward(x)\n",
        "        for layer in self.ordered_layers:\n",
        "            x = layer.forward(x)\n",
        "        x = self.dense.forward(x)\n",
        "        x = self.softmax.forward(x)\n",
        "        return x\n",
        "\n",
        "# 7. Compare Forward Propagation Results\n",
        "def compare_keras_and_scratch(keras_model, scratch_model, x_test, y_test):\n",
        "    y_keras = np.argmax(keras_model.predict(x_test), axis=1)\n",
        "\n",
        "    x_test_np = np.array(x_test)\n",
        "    y_scratch = []\n",
        "    for xi in x_test_np:\n",
        "        pred = scratch_model.forward(xi[np.newaxis, :])\n",
        "        y_scratch.append(np.argmax(pred, axis=1)[0])\n",
        "    y_scratch = np.array(y_scratch)\n",
        "\n",
        "    if y_test.ndim > 1:\n",
        "        y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    from sklearn.metrics import f1_score\n",
        "    keras_f1 = f1_score(y_test, y_keras, average=\"macro\")\n",
        "    scratch_f1 = f1_score(y_test, y_scratch, average=\"macro\")\n",
        "    print(f\"Keras F1: {keras_f1:.4f}, Scratch F1: {scratch_f1:.4f}\")\n",
        "\n",
        "# 8. Usage:\n",
        "keras_model = build_rnn_model(\n",
        "    vocab_size=10000, seq_len=100, embed_dim=64, rnn_units=64,\n",
        "    num_layers=2, dropout_rate=0.3, bidirectional=False, num_classes=3\n",
        ")\n",
        "keras_model(np.zeros((1, 100), dtype=np.int32))  \n",
        "keras_model.load_weights(\"nusax_rnn.weights.h5\")  \n",
        "\n",
        "scratch_model = MyRNNModel(keras_model)\n",
        "compare_keras_and_scratch(keras_model, scratch_model, x_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
